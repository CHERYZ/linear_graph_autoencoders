{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Masking test edges...\n",
      "Preprocessing and Initializing...\n",
      "WARNING:tensorflow:From E:\\jupyter\\linear_graph_autoencoders\\linear_gae\\train.py:177: The name tf.sparse_placeholder is deprecated. Please use tf.compat.v1.sparse_placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From E:\\jupyter\\linear_graph_autoencoders\\linear_gae\\train.py:180: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From e:\\project\\linear_graph_autoencoders\\linear_gae\\model.py:38: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From e:\\project\\linear_graph_autoencoders\\linear_gae\\initializations.py:14: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From e:\\project\\linear_graph_autoencoders\\linear_gae\\layers.py:29: The name tf.sparse_retain is deprecated. Please use tf.sparse.retain instead.\n",
      "\n",
      "WARNING:tensorflow:From e:\\project\\linear_graph_autoencoders\\linear_gae\\layers.py:101: The name tf.sparse_tensor_dense_matmul is deprecated. Please use tf.sparse.sparse_dense_matmul instead.\n",
      "\n",
      "WARNING:tensorflow:From e:\\project\\linear_graph_autoencoders\\linear_gae\\layers.py:79: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From e:\\project\\linear_graph_autoencoders\\linear_gae\\model.py:123: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "WARNING:tensorflow:From e:\\project\\linear_graph_autoencoders\\linear_gae\\model.py:40: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From e:\\project\\linear_graph_autoencoders\\linear_gae\\model.py:40: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "WARNING:tensorflow:From E:\\jupyter\\linear_graph_autoencoders\\linear_gae\\train.py:224: The name tf.sparse_tensor_to_dense is deprecated. Please use tf.sparse.to_dense instead.\n",
      "\n",
      "WARNING:tensorflow:From e:\\project\\linear_graph_autoencoders\\linear_gae\\optimizer.py:39: calling weighted_cross_entropy_with_logits (from tensorflow.python.ops.nn_impl) with targets is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "targets is deprecated, use labels instead\n",
      "WARNING:tensorflow:From e:\\project\\linear_graph_autoencoders\\linear_gae\\optimizer.py:41: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From E:\\jupyter\\linear_graph_autoencoders\\linear_gae\\train.py:236: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From E:\\jupyter\\linear_graph_autoencoders\\linear_gae\\train.py:237: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "Training...\n",
      "Epoch: 0001 train_loss= 1.70241 time= 0.61668\n",
      "Epoch: 0002 train_loss= 1.73338 time= 0.22897\n",
      "Epoch: 0003 train_loss= 1.74971 time= 0.21704\n",
      "Epoch: 0004 train_loss= 1.66285 time= 0.21904\n",
      "Epoch: 0005 train_loss= 1.68220 time= 0.22003\n",
      "Epoch: 0006 train_loss= 1.65817 time= 0.23596\n",
      "Epoch: 0007 train_loss= 1.60020 time= 0.22470\n",
      "Epoch: 0008 train_loss= 1.56314 time= 0.23366\n",
      "Epoch: 0009 train_loss= 1.52075 time= 0.24457\n",
      "Epoch: 0010 train_loss= 1.48776 time= 0.23973\n",
      "Epoch: 0011 train_loss= 1.42094 time= 0.24214\n",
      "Epoch: 0012 train_loss= 1.37288 time= 0.22426\n",
      "Epoch: 0013 train_loss= 1.31991 time= 0.23785\n",
      "Epoch: 0014 train_loss= 1.25885 time= 0.22205\n",
      "Epoch: 0015 train_loss= 1.18840 time= 0.23046\n",
      "Epoch: 0016 train_loss= 1.13016 time= 0.21812\n",
      "Epoch: 0017 train_loss= 1.08164 time= 0.21505\n",
      "Epoch: 0018 train_loss= 1.02367 time= 0.21879\n",
      "Epoch: 0019 train_loss= 0.98633 time= 0.22254\n",
      "Epoch: 0020 train_loss= 0.92852 time= 0.23771\n",
      "Epoch: 0021 train_loss= 0.89817 time= 0.23997\n",
      "Epoch: 0022 train_loss= 0.85654 time= 0.22306\n",
      "Epoch: 0023 train_loss= 0.81129 time= 0.21906\n",
      "Epoch: 0024 train_loss= 0.79651 time= 0.22652\n",
      "Epoch: 0025 train_loss= 0.77347 time= 0.23596\n",
      "Epoch: 0026 train_loss= 0.76289 time= 0.23984\n",
      "Epoch: 0027 train_loss= 0.74363 time= 0.21859\n",
      "Epoch: 0028 train_loss= 0.72878 time= 0.23954\n",
      "Epoch: 0029 train_loss= 0.71941 time= 0.23399\n",
      "Epoch: 0030 train_loss= 0.70893 time= 0.22102\n",
      "Epoch: 0031 train_loss= 0.70681 time= 0.24101\n",
      "Epoch: 0032 train_loss= 0.69955 time= 0.22202\n",
      "Epoch: 0033 train_loss= 0.69487 time= 0.24161\n",
      "Epoch: 0034 train_loss= 0.68840 time= 0.24457\n",
      "Epoch: 0035 train_loss= 0.68683 time= 0.23870\n",
      "Epoch: 0036 train_loss= 0.67971 time= 0.23596\n",
      "Epoch: 0037 train_loss= 0.67516 time= 0.24853\n",
      "Epoch: 0038 train_loss= 0.66520 time= 0.21855\n",
      "Epoch: 0039 train_loss= 0.65945 time= 0.23513\n",
      "Epoch: 0040 train_loss= 0.65597 time= 0.23707\n",
      "Epoch: 0041 train_loss= 0.64614 time= 0.22306\n",
      "Epoch: 0042 train_loss= 0.63627 time= 0.21596\n",
      "Epoch: 0043 train_loss= 0.62707 time= 0.21070\n",
      "Epoch: 0044 train_loss= 0.61656 time= 0.23309\n",
      "Epoch: 0045 train_loss= 0.60864 time= 0.21996\n",
      "Epoch: 0046 train_loss= 0.59390 time= 0.21772\n",
      "Epoch: 0047 train_loss= 0.58365 time= 0.24801\n",
      "Epoch: 0048 train_loss= 0.57132 time= 0.22850\n",
      "Epoch: 0049 train_loss= 0.56167 time= 0.22091\n",
      "Epoch: 0050 train_loss= 0.55421 time= 0.21704\n",
      "Epoch: 0051 train_loss= 0.54679 time= 0.21505\n",
      "Epoch: 0052 train_loss= 0.54263 time= 0.24193\n",
      "Epoch: 0053 train_loss= 0.53926 time= 0.24332\n",
      "Epoch: 0054 train_loss= 0.53519 time= 0.23995\n",
      "Epoch: 0055 train_loss= 0.53298 time= 0.23295\n",
      "Epoch: 0056 train_loss= 0.52966 time= 0.22900\n",
      "Epoch: 0057 train_loss= 0.52588 time= 0.23696\n",
      "Epoch: 0058 train_loss= 0.52409 time= 0.24095\n",
      "Epoch: 0059 train_loss= 0.51944 time= 0.23883\n",
      "Epoch: 0060 train_loss= 0.51414 time= 0.23497\n",
      "Epoch: 0061 train_loss= 0.50893 time= 0.23862\n",
      "Epoch: 0062 train_loss= 0.50408 time= 0.22254\n",
      "Epoch: 0063 train_loss= 0.50131 time= 0.23742\n",
      "Epoch: 0064 train_loss= 0.49717 time= 0.21577\n",
      "Epoch: 0065 train_loss= 0.49384 time= 0.24743\n",
      "Epoch: 0066 train_loss= 0.49084 time= 0.22005\n",
      "Epoch: 0067 train_loss= 0.48807 time= 0.22105\n",
      "Epoch: 0068 train_loss= 0.48659 time= 0.22103\n",
      "Epoch: 0069 train_loss= 0.48488 time= 0.22017\n",
      "Epoch: 0070 train_loss= 0.48312 time= 0.24553\n",
      "Epoch: 0071 train_loss= 0.48103 time= 0.22304\n",
      "Epoch: 0072 train_loss= 0.47989 time= 0.21907\n",
      "Epoch: 0073 train_loss= 0.47793 time= 0.23696\n",
      "Epoch: 0074 train_loss= 0.47581 time= 0.22253\n",
      "Epoch: 0075 train_loss= 0.47471 time= 0.24110\n",
      "Epoch: 0076 train_loss= 0.47232 time= 0.23939\n",
      "Epoch: 0077 train_loss= 0.47095 time= 0.24595\n",
      "Epoch: 0078 train_loss= 0.47039 time= 0.24325\n",
      "Epoch: 0079 train_loss= 0.46869 time= 0.23801\n",
      "Epoch: 0080 train_loss= 0.46810 time= 0.24401\n",
      "Epoch: 0081 train_loss= 0.46742 time= 0.24149\n",
      "Epoch: 0082 train_loss= 0.46666 time= 0.22103\n",
      "Epoch: 0083 train_loss= 0.46502 time= 0.22202\n",
      "Epoch: 0084 train_loss= 0.46457 time= 0.21855\n",
      "Epoch: 0085 train_loss= 0.46361 time= 0.22465\n",
      "Epoch: 0086 train_loss= 0.46230 time= 0.21676\n",
      "Epoch: 0087 train_loss= 0.46156 time= 0.21756\n",
      "Epoch: 0088 train_loss= 0.46115 time= 0.22800\n",
      "Epoch: 0089 train_loss= 0.46025 time= 0.22107\n",
      "Epoch: 0090 train_loss= 0.45937 time= 0.21601\n",
      "Epoch: 0091 train_loss= 0.45856 time= 0.21605\n",
      "Epoch: 0092 train_loss= 0.45810 time= 0.22804\n",
      "Epoch: 0093 train_loss= 0.45758 time= 0.22226\n",
      "Epoch: 0094 train_loss= 0.45693 time= 0.23854\n",
      "Epoch: 0095 train_loss= 0.45571 time= 0.24147\n",
      "Epoch: 0096 train_loss= 0.45577 time= 0.23999\n",
      "Epoch: 0097 train_loss= 0.45485 time= 0.22692\n",
      "Epoch: 0098 train_loss= 0.45410 time= 0.24444\n",
      "Epoch: 0099 train_loss= 0.45374 time= 0.23149\n",
      "Epoch: 0100 train_loss= 0.45253 time= 0.22008\n",
      "Epoch: 0101 train_loss= 0.45189 time= 0.24442\n",
      "Epoch: 0102 train_loss= 0.45188 time= 0.22501\n",
      "Epoch: 0103 train_loss= 0.45147 time= 0.21659\n",
      "Epoch: 0104 train_loss= 0.45036 time= 0.22006\n",
      "Epoch: 0105 train_loss= 0.45027 time= 0.22700\n",
      "Epoch: 0106 train_loss= 0.44932 time= 0.22348\n",
      "Epoch: 0107 train_loss= 0.44945 time= 0.24505\n",
      "Epoch: 0108 train_loss= 0.44902 time= 0.23806\n",
      "Epoch: 0109 train_loss= 0.44837 time= 0.23998\n",
      "Epoch: 0110 train_loss= 0.44728 time= 0.23719\n",
      "Epoch: 0111 train_loss= 0.44749 time= 0.23548\n",
      "Epoch: 0112 train_loss= 0.44675 time= 0.21457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0113 train_loss= 0.44600 time= 0.22463\n",
      "Epoch: 0114 train_loss= 0.44616 time= 0.21318\n",
      "Epoch: 0115 train_loss= 0.44558 time= 0.21899\n",
      "Epoch: 0116 train_loss= 0.44503 time= 0.22256\n",
      "Epoch: 0117 train_loss= 0.44432 time= 0.22453\n",
      "Epoch: 0118 train_loss= 0.44447 time= 0.23200\n",
      "Epoch: 0119 train_loss= 0.44388 time= 0.22082\n",
      "Epoch: 0120 train_loss= 0.44374 time= 0.21758\n",
      "Epoch: 0121 train_loss= 0.44320 time= 0.22402\n",
      "Epoch: 0122 train_loss= 0.44296 time= 0.22026\n",
      "Epoch: 0123 train_loss= 0.44263 time= 0.21556\n",
      "Epoch: 0124 train_loss= 0.44183 time= 0.21409\n",
      "Epoch: 0125 train_loss= 0.44193 time= 0.22440\n",
      "Epoch: 0126 train_loss= 0.44178 time= 0.21767\n",
      "Epoch: 0127 train_loss= 0.44123 time= 0.21573\n",
      "Epoch: 0128 train_loss= 0.44105 time= 0.22154\n",
      "Epoch: 0129 train_loss= 0.44043 time= 0.22102\n",
      "Epoch: 0130 train_loss= 0.44043 time= 0.23693\n",
      "Epoch: 0131 train_loss= 0.44018 time= 0.23773\n",
      "Epoch: 0132 train_loss= 0.43962 time= 0.22341\n",
      "Epoch: 0133 train_loss= 0.43931 time= 0.21949\n",
      "Epoch: 0134 train_loss= 0.43918 time= 0.22995\n",
      "Epoch: 0135 train_loss= 0.43893 time= 0.22205\n",
      "Epoch: 0136 train_loss= 0.43855 time= 0.21990\n",
      "Epoch: 0137 train_loss= 0.43869 time= 0.23300\n",
      "Epoch: 0138 train_loss= 0.43816 time= 0.23473\n",
      "Epoch: 0139 train_loss= 0.43806 time= 0.23812\n",
      "Epoch: 0140 train_loss= 0.43735 time= 0.22531\n",
      "Epoch: 0141 train_loss= 0.43743 time= 0.22455\n",
      "Epoch: 0142 train_loss= 0.43731 time= 0.24293\n",
      "Epoch: 0143 train_loss= 0.43690 time= 0.23969\n",
      "Epoch: 0144 train_loss= 0.43691 time= 0.22245\n",
      "Epoch: 0145 train_loss= 0.43646 time= 0.23699\n",
      "Epoch: 0146 train_loss= 0.43621 time= 0.23950\n",
      "Epoch: 0147 train_loss= 0.43623 time= 0.22552\n",
      "Epoch: 0148 train_loss= 0.43595 time= 0.23705\n",
      "Epoch: 0149 train_loss= 0.43565 time= 0.24060\n",
      "Epoch: 0150 train_loss= 0.43565 time= 0.23998\n",
      "Epoch: 0151 train_loss= 0.43539 time= 0.24101\n",
      "Epoch: 0152 train_loss= 0.43489 time= 0.23999\n",
      "Epoch: 0153 train_loss= 0.43488 time= 0.24408\n",
      "Epoch: 0154 train_loss= 0.43487 time= 0.23915\n",
      "Epoch: 0155 train_loss= 0.43459 time= 0.22308\n",
      "Epoch: 0156 train_loss= 0.43424 time= 0.22359\n",
      "Epoch: 0157 train_loss= 0.43406 time= 0.23655\n",
      "Epoch: 0158 train_loss= 0.43432 time= 0.24446\n",
      "Epoch: 0159 train_loss= 0.43412 time= 0.22321\n",
      "Epoch: 0160 train_loss= 0.43397 time= 0.21209\n",
      "Epoch: 0161 train_loss= 0.43364 time= 0.23596\n",
      "Epoch: 0162 train_loss= 0.43342 time= 0.23124\n",
      "Epoch: 0163 train_loss= 0.43308 time= 0.23745\n",
      "Epoch: 0164 train_loss= 0.43337 time= 0.24363\n",
      "Epoch: 0165 train_loss= 0.43294 time= 0.22600\n",
      "Epoch: 0166 train_loss= 0.43299 time= 0.22501\n",
      "Epoch: 0167 train_loss= 0.43285 time= 0.21605\n",
      "Epoch: 0168 train_loss= 0.43270 time= 0.21976\n",
      "Epoch: 0169 train_loss= 0.43254 time= 0.21953\n",
      "Epoch: 0170 train_loss= 0.43241 time= 0.23656\n",
      "Epoch: 0171 train_loss= 0.43206 time= 0.24115\n",
      "Epoch: 0172 train_loss= 0.43191 time= 0.24410\n",
      "Epoch: 0173 train_loss= 0.43181 time= 0.23247\n",
      "Epoch: 0174 train_loss= 0.43166 time= 0.23900\n",
      "Epoch: 0175 train_loss= 0.43164 time= 0.23774\n",
      "Epoch: 0176 train_loss= 0.43159 time= 0.24378\n",
      "Epoch: 0177 train_loss= 0.43113 time= 0.22401\n",
      "Epoch: 0178 train_loss= 0.43103 time= 0.21936\n",
      "Epoch: 0179 train_loss= 0.43106 time= 0.21804\n",
      "Epoch: 0180 train_loss= 0.43073 time= 0.21446\n",
      "Epoch: 0181 train_loss= 0.43071 time= 0.23819\n",
      "Epoch: 0182 train_loss= 0.43056 time= 0.22122\n",
      "Epoch: 0183 train_loss= 0.43059 time= 0.21408\n",
      "Epoch: 0184 train_loss= 0.43041 time= 0.23795\n",
      "Epoch: 0185 train_loss= 0.43037 time= 0.22603\n",
      "Epoch: 0186 train_loss= 0.43005 time= 0.24259\n",
      "Epoch: 0187 train_loss= 0.42998 time= 0.22653\n",
      "Epoch: 0188 train_loss= 0.42977 time= 0.22119\n",
      "Epoch: 0189 train_loss= 0.42966 time= 0.22401\n",
      "Epoch: 0190 train_loss= 0.42978 time= 0.22353\n",
      "Epoch: 0191 train_loss= 0.42961 time= 0.22452\n",
      "Epoch: 0192 train_loss= 0.42924 time= 0.21505\n",
      "Epoch: 0193 train_loss= 0.42900 time= 0.21876\n",
      "Epoch: 0194 train_loss= 0.42910 time= 0.22397\n",
      "Epoch: 0195 train_loss= 0.42888 time= 0.21878\n",
      "Epoch: 0196 train_loss= 0.42872 time= 0.21707\n",
      "Epoch: 0197 train_loss= 0.42862 time= 0.22103\n",
      "Epoch: 0198 train_loss= 0.42828 time= 0.22439\n",
      "Epoch: 0199 train_loss= 0.42841 time= 0.23746\n",
      "Epoch: 0200 train_loss= 0.42845 time= 0.23547\n",
      "Testing model...\n",
      "\n",
      "Test results for gcn_vae model on cora on link_prediction \n",
      " ___________________________________________________\n",
      "\n",
      "AUC scores\n",
      " [0.8331539018251606]\n",
      "Mean AUC score:  0.8331539018251606 \n",
      "Std of AUC scores:  0.0 \n",
      " \n",
      "\n",
      "AP scores\n",
      " [0.8722161472896266]\n",
      "Mean AP score:  0.8722161472896266 \n",
      "Std of AP scores:  0.0 \n",
      " \n",
      "\n",
      "Total Running times\n",
      " [47.143205881118774]\n",
      "Mean total running time:  47.143205881118774 \n",
      "Std of total running time:  0.0 \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run train.py --model=gcn_vae --dataset=cora --task=link_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Masking test edges...\n",
      "Preprocessing and Initializing...\n",
      "Training...\n",
      "Epoch: 0001 train_loss= 1.69649 time= 0.54284\n",
      "Epoch: 0002 train_loss= 1.70568 time= 0.21536\n",
      "Epoch: 0003 train_loss= 1.68877 time= 0.22226\n",
      "Epoch: 0004 train_loss= 1.66173 time= 0.22123\n",
      "Epoch: 0005 train_loss= 1.65267 time= 0.22766\n",
      "Epoch: 0006 train_loss= 1.61148 time= 0.23002\n",
      "Epoch: 0007 train_loss= 1.60789 time= 0.22317\n",
      "Epoch: 0008 train_loss= 1.59724 time= 0.21605\n",
      "Epoch: 0009 train_loss= 1.55989 time= 0.21905\n",
      "Epoch: 0010 train_loss= 1.57508 time= 0.21206\n",
      "Epoch: 0011 train_loss= 1.55792 time= 0.21505\n",
      "Epoch: 0012 train_loss= 1.51223 time= 0.22254\n",
      "Epoch: 0013 train_loss= 1.50636 time= 0.22342\n",
      "Epoch: 0014 train_loss= 1.48897 time= 0.22495\n",
      "Epoch: 0015 train_loss= 1.49917 time= 0.23167\n",
      "Epoch: 0016 train_loss= 1.45665 time= 0.21981\n",
      "Epoch: 0017 train_loss= 1.44236 time= 0.22034\n",
      "Epoch: 0018 train_loss= 1.44451 time= 0.22890\n",
      "Epoch: 0019 train_loss= 1.43053 time= 0.22674\n",
      "Epoch: 0020 train_loss= 1.43080 time= 0.22395\n",
      "Epoch: 0021 train_loss= 1.39557 time= 0.22457\n",
      "Epoch: 0022 train_loss= 1.36944 time= 0.22660\n",
      "Epoch: 0023 train_loss= 1.38310 time= 0.22201\n",
      "Epoch: 0024 train_loss= 1.37503 time= 0.22811\n",
      "Epoch: 0025 train_loss= 1.35765 time= 0.21804\n",
      "Epoch: 0026 train_loss= 1.33553 time= 0.22210\n",
      "Epoch: 0027 train_loss= 1.32576 time= 0.23110\n",
      "Epoch: 0028 train_loss= 1.30725 time= 0.21687\n",
      "Epoch: 0029 train_loss= 1.29911 time= 0.22850\n",
      "Epoch: 0030 train_loss= 1.29231 time= 0.22276\n",
      "Epoch: 0031 train_loss= 1.29019 time= 0.23353\n",
      "Epoch: 0032 train_loss= 1.24768 time= 0.22234\n",
      "Epoch: 0033 train_loss= 1.25465 time= 0.22873\n",
      "Epoch: 0034 train_loss= 1.26609 time= 0.22886\n",
      "Epoch: 0035 train_loss= 1.24266 time= 0.22167\n",
      "Epoch: 0036 train_loss= 1.21732 time= 0.21930\n",
      "Epoch: 0037 train_loss= 1.24960 time= 0.21576\n",
      "Epoch: 0038 train_loss= 1.20017 time= 0.21931\n",
      "Epoch: 0039 train_loss= 1.19064 time= 0.21951\n",
      "Epoch: 0040 train_loss= 1.19987 time= 0.22210\n",
      "Epoch: 0041 train_loss= 1.21514 time= 0.21605\n",
      "Epoch: 0042 train_loss= 1.20580 time= 0.21804\n",
      "Epoch: 0043 train_loss= 1.18748 time= 0.22003\n",
      "Epoch: 0044 train_loss= 1.17763 time= 0.21505\n",
      "Epoch: 0045 train_loss= 1.15778 time= 0.22506\n",
      "Epoch: 0046 train_loss= 1.14117 time= 0.22003\n",
      "Epoch: 0047 train_loss= 1.15350 time= 0.23161\n",
      "Epoch: 0048 train_loss= 1.15361 time= 0.21893\n",
      "Epoch: 0049 train_loss= 1.13548 time= 0.21804\n",
      "Epoch: 0050 train_loss= 1.12644 time= 0.23733\n",
      "Epoch: 0051 train_loss= 1.12101 time= 0.22999\n",
      "Epoch: 0052 train_loss= 1.11768 time= 0.22302\n",
      "Epoch: 0053 train_loss= 1.10307 time= 0.23351\n",
      "Epoch: 0054 train_loss= 1.09699 time= 0.23098\n",
      "Epoch: 0055 train_loss= 1.08838 time= 0.21457\n",
      "Epoch: 0056 train_loss= 1.08480 time= 0.22205\n",
      "Epoch: 0057 train_loss= 1.08389 time= 0.21906\n",
      "Epoch: 0058 train_loss= 1.05264 time= 0.21904\n",
      "Epoch: 0059 train_loss= 1.06140 time= 0.22600\n",
      "Epoch: 0060 train_loss= 1.06848 time= 0.21693\n",
      "Epoch: 0061 train_loss= 1.04926 time= 0.21804\n",
      "Epoch: 0062 train_loss= 1.04663 time= 0.22009\n",
      "Epoch: 0063 train_loss= 1.04760 time= 0.22203\n",
      "Epoch: 0064 train_loss= 1.02635 time= 0.20759\n",
      "Epoch: 0065 train_loss= 1.04063 time= 0.22403\n",
      "Epoch: 0066 train_loss= 1.01059 time= 0.22202\n",
      "Epoch: 0067 train_loss= 1.02540 time= 0.21903\n",
      "Epoch: 0068 train_loss= 1.01110 time= 0.22401\n",
      "Epoch: 0069 train_loss= 1.00551 time= 0.22161\n",
      "Epoch: 0070 train_loss= 0.99226 time= 0.22257\n",
      "Epoch: 0071 train_loss= 0.99000 time= 0.21704\n",
      "Epoch: 0072 train_loss= 0.99748 time= 0.21704\n",
      "Epoch: 0073 train_loss= 0.97480 time= 0.22253\n",
      "Epoch: 0074 train_loss= 0.98697 time= 0.21910\n",
      "Epoch: 0075 train_loss= 0.97479 time= 0.21583\n",
      "Epoch: 0076 train_loss= 0.97706 time= 0.21756\n",
      "Epoch: 0077 train_loss= 0.97757 time= 0.22703\n",
      "Epoch: 0078 train_loss= 0.96732 time= 0.21906\n",
      "Epoch: 0079 train_loss= 0.96961 time= 0.22006\n",
      "Epoch: 0080 train_loss= 0.95089 time= 0.22501\n",
      "Epoch: 0081 train_loss= 0.95599 time= 0.22600\n",
      "Epoch: 0082 train_loss= 0.93754 time= 0.22202\n",
      "Epoch: 0083 train_loss= 0.95299 time= 0.22198\n",
      "Epoch: 0084 train_loss= 0.91426 time= 0.22353\n",
      "Epoch: 0085 train_loss= 0.90942 time= 0.23179\n",
      "Epoch: 0086 train_loss= 0.92669 time= 0.22099\n",
      "Epoch: 0087 train_loss= 0.91529 time= 0.22651\n",
      "Epoch: 0088 train_loss= 0.91973 time= 0.21204\n",
      "Epoch: 0089 train_loss= 0.90434 time= 0.21407\n",
      "Epoch: 0090 train_loss= 0.89650 time= 0.21804\n",
      "Epoch: 0091 train_loss= 0.90011 time= 0.21903\n",
      "Epoch: 0092 train_loss= 0.89543 time= 0.21755\n",
      "Epoch: 0093 train_loss= 0.87578 time= 0.21909\n",
      "Epoch: 0094 train_loss= 0.86497 time= 0.22245\n",
      "Epoch: 0095 train_loss= 0.86218 time= 0.21955\n",
      "Epoch: 0096 train_loss= 0.87642 time= 0.22573\n",
      "Epoch: 0097 train_loss= 0.86541 time= 0.22295\n",
      "Epoch: 0098 train_loss= 0.85651 time= 0.22772\n",
      "Epoch: 0099 train_loss= 0.85230 time= 0.22049\n",
      "Epoch: 0100 train_loss= 0.83829 time= 0.22283\n",
      "Epoch: 0101 train_loss= 0.84137 time= 0.22353\n",
      "Epoch: 0102 train_loss= 0.84635 time= 0.21755\n",
      "Epoch: 0103 train_loss= 0.83942 time= 0.22202\n",
      "Epoch: 0104 train_loss= 0.83694 time= 0.21381\n",
      "Epoch: 0105 train_loss= 0.82007 time= 0.21700\n",
      "Epoch: 0106 train_loss= 0.82385 time= 0.22736\n",
      "Epoch: 0107 train_loss= 0.81528 time= 0.22467\n",
      "Epoch: 0108 train_loss= 0.80494 time= 0.21434\n",
      "Epoch: 0109 train_loss= 0.80556 time= 0.21559\n",
      "Epoch: 0110 train_loss= 0.80464 time= 0.22984\n",
      "Epoch: 0111 train_loss= 0.78932 time= 0.23098\n",
      "Epoch: 0112 train_loss= 0.78496 time= 0.22302\n",
      "Epoch: 0113 train_loss= 0.77959 time= 0.22007\n",
      "Epoch: 0114 train_loss= 0.78428 time= 0.21973\n",
      "Epoch: 0115 train_loss= 0.78151 time= 0.21035\n",
      "Epoch: 0116 train_loss= 0.77047 time= 0.21855\n",
      "Epoch: 0117 train_loss= 0.76626 time= 0.21388\n",
      "Epoch: 0118 train_loss= 0.76005 time= 0.22129\n",
      "Epoch: 0119 train_loss= 0.75765 time= 0.22585\n",
      "Epoch: 0120 train_loss= 0.75672 time= 0.21928\n",
      "Epoch: 0121 train_loss= 0.74173 time= 0.22660\n",
      "Epoch: 0122 train_loss= 0.74759 time= 0.22415\n",
      "Epoch: 0123 train_loss= 0.73886 time= 0.21672\n",
      "Epoch: 0124 train_loss= 0.74068 time= 0.21810\n",
      "Epoch: 0125 train_loss= 0.72924 time= 0.22281\n",
      "Epoch: 0126 train_loss= 0.73266 time= 0.21383\n",
      "Epoch: 0127 train_loss= 0.72344 time= 0.22706\n",
      "Epoch: 0128 train_loss= 0.71420 time= 0.20908\n",
      "Epoch: 0129 train_loss= 0.71133 time= 0.22302\n",
      "Epoch: 0130 train_loss= 0.71182 time= 0.22104\n",
      "Epoch: 0131 train_loss= 0.70477 time= 0.21908\n",
      "Epoch: 0132 train_loss= 0.70225 time= 0.21443\n",
      "Epoch: 0133 train_loss= 0.70402 time= 0.22901\n",
      "Epoch: 0134 train_loss= 0.69022 time= 0.22857\n",
      "Epoch: 0135 train_loss= 0.69019 time= 0.21567\n",
      "Epoch: 0136 train_loss= 0.69056 time= 0.22698\n",
      "Epoch: 0137 train_loss= 0.68227 time= 0.22285\n",
      "Epoch: 0138 train_loss= 0.68393 time= 0.22018\n",
      "Epoch: 0139 train_loss= 0.67500 time= 0.21875\n",
      "Epoch: 0140 train_loss= 0.67198 time= 0.22360\n",
      "Epoch: 0141 train_loss= 0.67440 time= 0.22160\n",
      "Epoch: 0142 train_loss= 0.67277 time= 0.22408\n",
      "Epoch: 0143 train_loss= 0.67081 time= 0.21960\n",
      "Epoch: 0144 train_loss= 0.66245 time= 0.22343\n",
      "Epoch: 0145 train_loss= 0.66480 time= 0.21705\n",
      "Epoch: 0146 train_loss= 0.64769 time= 0.22003\n",
      "Epoch: 0147 train_loss= 0.65663 time= 0.21406\n",
      "Epoch: 0148 train_loss= 0.64683 time= 0.21306\n",
      "Epoch: 0149 train_loss= 0.65148 time= 0.21507\n",
      "Epoch: 0150 train_loss= 0.64687 time= 0.21980\n",
      "Epoch: 0151 train_loss= 0.64207 time= 0.22797\n",
      "Epoch: 0152 train_loss= 0.64004 time= 0.21592\n",
      "Epoch: 0153 train_loss= 0.63927 time= 0.22421\n",
      "Epoch: 0154 train_loss= 0.63305 time= 0.22386\n",
      "Epoch: 0155 train_loss= 0.63744 time= 0.22567\n",
      "Epoch: 0156 train_loss= 0.63765 time= 0.22616\n",
      "Epoch: 0157 train_loss= 0.62789 time= 0.22003\n",
      "Epoch: 0158 train_loss= 0.62745 time= 0.22003\n",
      "Epoch: 0159 train_loss= 0.62550 time= 0.21855\n",
      "Epoch: 0160 train_loss= 0.62346 time= 0.22208\n",
      "Epoch: 0161 train_loss= 0.61598 time= 0.22302\n",
      "Epoch: 0162 train_loss= 0.61904 time= 0.22303\n",
      "Epoch: 0163 train_loss= 0.62191 time= 0.21805\n",
      "Epoch: 0164 train_loss= 0.62122 time= 0.21506\n",
      "Epoch: 0165 train_loss= 0.61128 time= 0.21158\n",
      "Epoch: 0166 train_loss= 0.60767 time= 0.22654\n",
      "Epoch: 0167 train_loss= 0.61075 time= 0.22536\n",
      "Epoch: 0168 train_loss= 0.60716 time= 0.21829\n",
      "Epoch: 0169 train_loss= 0.60989 time= 0.22234\n",
      "Epoch: 0170 train_loss= 0.60235 time= 0.21510\n",
      "Epoch: 0171 train_loss= 0.59613 time= 0.22357\n",
      "Epoch: 0172 train_loss= 0.60093 time= 0.22554\n",
      "Epoch: 0173 train_loss= 0.60233 time= 0.22303\n",
      "Epoch: 0174 train_loss= 0.59099 time= 0.22302\n",
      "Epoch: 0175 train_loss= 0.60044 time= 0.21505\n",
      "Epoch: 0176 train_loss= 0.59531 time= 0.21806\n",
      "Epoch: 0177 train_loss= 0.59450 time= 0.22999\n",
      "Epoch: 0178 train_loss= 0.59745 time= 0.21844\n",
      "Epoch: 0179 train_loss= 0.59418 time= 0.21699\n",
      "Epoch: 0180 train_loss= 0.58798 time= 0.22903\n",
      "Epoch: 0181 train_loss= 0.59065 time= 0.21071\n",
      "Epoch: 0182 train_loss= 0.59066 time= 0.21461\n",
      "Epoch: 0183 train_loss= 0.58387 time= 0.22206\n",
      "Epoch: 0184 train_loss= 0.58449 time= 0.22018\n",
      "Epoch: 0185 train_loss= 0.58449 time= 0.22154\n",
      "Epoch: 0186 train_loss= 0.58401 time= 0.21705\n",
      "Epoch: 0187 train_loss= 0.57891 time= 0.21914\n",
      "Epoch: 0188 train_loss= 0.57846 time= 0.21804\n",
      "Epoch: 0189 train_loss= 0.57099 time= 0.22838\n",
      "Epoch: 0190 train_loss= 0.57728 time= 0.21810\n",
      "Epoch: 0191 train_loss= 0.57607 time= 0.21971\n",
      "Epoch: 0192 train_loss= 0.57966 time= 0.22775\n",
      "Epoch: 0193 train_loss= 0.57710 time= 0.21727\n",
      "Epoch: 0194 train_loss= 0.57581 time= 0.22159\n",
      "Epoch: 0195 train_loss= 0.57843 time= 0.21649\n",
      "Epoch: 0196 train_loss= 0.57289 time= 0.21541\n",
      "Epoch: 0197 train_loss= 0.57455 time= 0.22255\n",
      "Epoch: 0198 train_loss= 0.56660 time= 0.22698\n",
      "Epoch: 0199 train_loss= 0.57241 time= 0.22849\n",
      "Epoch: 0200 train_loss= 0.56596 time= 0.22500\n",
      "Testing model...\n",
      "\n",
      "Test results for linear_vae model on cora on link_prediction \n",
      " ___________________________________________________\n",
      "\n",
      "AUC scores\n",
      " [0.8713962171757361]\n",
      "Mean AUC score:  0.8713962171757361 \n",
      "Std of AUC scores:  0.0 \n",
      " \n",
      "\n",
      "AP scores\n",
      " [0.8982342973906562]\n",
      "Mean AP score:  0.8982342973906562 \n",
      "Std of AP scores:  0.0 \n",
      " \n",
      "\n",
      "Total Running times\n",
      " [45.05121636390686]\n",
      "Mean total running time:  45.05121636390686 \n",
      "Std of total running time:  0.0 \n",
      " \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From train.py:177: The name tf.sparse_placeholder is deprecated. Please use tf.compat.v1.sparse_placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From train.py:180: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From e:\\project\\linear_graph_autoencoders\\linear_gae\\model.py:38: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From e:\\project\\linear_graph_autoencoders\\linear_gae\\initializations.py:14: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From e:\\project\\linear_graph_autoencoders\\linear_gae\\layers.py:29: The name tf.sparse_retain is deprecated. Please use tf.sparse.retain instead.\n",
      "\n",
      "WARNING:tensorflow:From e:\\project\\linear_graph_autoencoders\\linear_gae\\layers.py:101: The name tf.sparse_tensor_dense_matmul is deprecated. Please use tf.sparse.sparse_dense_matmul instead.\n",
      "\n",
      "WARNING:tensorflow:From e:\\project\\linear_graph_autoencoders\\linear_gae\\model.py:192: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "WARNING:tensorflow:From e:\\project\\linear_graph_autoencoders\\linear_gae\\layers.py:115: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From e:\\project\\linear_graph_autoencoders\\linear_gae\\model.py:40: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From e:\\project\\linear_graph_autoencoders\\linear_gae\\model.py:40: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "WARNING:tensorflow:From train.py:224: The name tf.sparse_tensor_to_dense is deprecated. Please use tf.sparse.to_dense instead.\n",
      "\n",
      "WARNING:tensorflow:From e:\\project\\linear_graph_autoencoders\\linear_gae\\optimizer.py:39: calling weighted_cross_entropy_with_logits (from tensorflow.python.ops.nn_impl) with targets is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "targets is deprecated, use labels instead\n",
      "WARNING:tensorflow:From e:\\project\\linear_graph_autoencoders\\linear_gae\\optimizer.py:41: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From train.py:236: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From train.py:237: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python train.py --model=linear_vae --dataset=cora --task=link_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Masking test edges...\n",
      "Preprocessing and Initializing...\n",
      "Training...\n",
      "Epoch: 0001 train_loss= 0.79703 time= 0.28674\n",
      "Epoch: 0002 train_loss= 0.79629 time= 0.20860\n",
      "Epoch: 0003 train_loss= 0.79504 time= 0.21058\n",
      "Epoch: 0004 train_loss= 0.79323 time= 0.21007\n",
      "Epoch: 0005 train_loss= 0.79080 time= 0.21306\n",
      "Epoch: 0006 train_loss= 0.78773 time= 0.24344\n",
      "Epoch: 0007 train_loss= 0.78397 time= 0.23198\n",
      "Epoch: 0008 train_loss= 0.77951 time= 0.20978\n",
      "Epoch: 0009 train_loss= 0.77431 time= 0.20908\n",
      "Epoch: 0010 train_loss= 0.76838 time= 0.23549\n",
      "Epoch: 0011 train_loss= 0.76169 time= 0.22404\n",
      "Epoch: 0012 train_loss= 0.75426 time= 0.23356\n",
      "Epoch: 0013 train_loss= 0.74609 time= 0.20809\n",
      "Epoch: 0014 train_loss= 0.73720 time= 0.22751\n",
      "Epoch: 0015 train_loss= 0.72763 time= 0.23001\n",
      "Epoch: 0016 train_loss= 0.71741 time= 0.23751\n",
      "Epoch: 0017 train_loss= 0.70659 time= 0.20859\n",
      "Epoch: 0018 train_loss= 0.69522 time= 0.22401\n",
      "Epoch: 0019 train_loss= 0.68339 time= 0.23496\n",
      "Epoch: 0020 train_loss= 0.67115 time= 0.23596\n",
      "Epoch: 0021 train_loss= 0.65859 time= 0.23386\n",
      "Epoch: 0022 train_loss= 0.64581 time= 0.23633\n",
      "Epoch: 0023 train_loss= 0.63288 time= 0.21257\n",
      "Epoch: 0024 train_loss= 0.61991 time= 0.21954\n",
      "Epoch: 0025 train_loss= 0.60698 time= 0.22899\n",
      "Epoch: 0026 train_loss= 0.59419 time= 0.22662\n",
      "Epoch: 0027 train_loss= 0.58162 time= 0.23050\n",
      "Epoch: 0028 train_loss= 0.56934 time= 0.23551\n",
      "Epoch: 0029 train_loss= 0.55743 time= 0.20811\n",
      "Epoch: 0030 train_loss= 0.54597 time= 0.20663\n",
      "Epoch: 0031 train_loss= 0.53499 time= 0.21212\n",
      "Epoch: 0032 train_loss= 0.52454 time= 0.20410\n",
      "Epoch: 0033 train_loss= 0.51467 time= 0.22985\n",
      "Epoch: 0034 train_loss= 0.50539 time= 0.22807\n",
      "Epoch: 0035 train_loss= 0.49672 time= 0.20979\n",
      "Epoch: 0036 train_loss= 0.48866 time= 0.20900\n",
      "Epoch: 0037 train_loss= 0.48122 time= 0.22804\n",
      "Epoch: 0038 train_loss= 0.47437 time= 0.21059\n",
      "Epoch: 0039 train_loss= 0.46810 time= 0.23050\n",
      "Epoch: 0040 train_loss= 0.46239 time= 0.23145\n",
      "Epoch: 0041 train_loss= 0.45722 time= 0.23362\n",
      "Epoch: 0042 train_loss= 0.45254 time= 0.23051\n",
      "Epoch: 0043 train_loss= 0.44834 time= 0.20408\n",
      "Epoch: 0044 train_loss= 0.44456 time= 0.20665\n",
      "Epoch: 0045 train_loss= 0.44120 time= 0.21066\n",
      "Epoch: 0046 train_loss= 0.43820 time= 0.20960\n",
      "Epoch: 0047 train_loss= 0.43553 time= 0.23336\n",
      "Epoch: 0048 train_loss= 0.43317 time= 0.22640\n",
      "Epoch: 0049 train_loss= 0.43108 time= 0.23431\n",
      "Epoch: 0050 train_loss= 0.42923 time= 0.22359\n",
      "Epoch: 0051 train_loss= 0.42760 time= 0.20633\n",
      "Epoch: 0052 train_loss= 0.42616 time= 0.20232\n",
      "Epoch: 0053 train_loss= 0.42489 time= 0.20519\n",
      "Epoch: 0054 train_loss= 0.42377 time= 0.20464\n",
      "Epoch: 0055 train_loss= 0.42277 time= 0.21211\n",
      "Epoch: 0056 train_loss= 0.42189 time= 0.21624\n",
      "Epoch: 0057 train_loss= 0.42110 time= 0.22896\n",
      "Epoch: 0058 train_loss= 0.42039 time= 0.20826\n",
      "Epoch: 0059 train_loss= 0.41976 time= 0.20711\n",
      "Epoch: 0060 train_loss= 0.41918 time= 0.22867\n",
      "Epoch: 0061 train_loss= 0.41866 time= 0.20911\n",
      "Epoch: 0062 train_loss= 0.41818 time= 0.20410\n",
      "Epoch: 0063 train_loss= 0.41773 time= 0.20711\n",
      "Epoch: 0064 train_loss= 0.41731 time= 0.22703\n",
      "Epoch: 0065 train_loss= 0.41692 time= 0.20859\n",
      "Epoch: 0066 train_loss= 0.41656 time= 0.22802\n",
      "Epoch: 0067 train_loss= 0.41621 time= 0.22781\n",
      "Epoch: 0068 train_loss= 0.41587 time= 0.22928\n",
      "Epoch: 0069 train_loss= 0.41555 time= 0.20032\n",
      "Epoch: 0070 train_loss= 0.41524 time= 0.22355\n",
      "Epoch: 0071 train_loss= 0.41494 time= 0.22679\n",
      "Epoch: 0072 train_loss= 0.41466 time= 0.22848\n",
      "Epoch: 0073 train_loss= 0.41438 time= 0.22904\n",
      "Epoch: 0074 train_loss= 0.41411 time= 0.23505\n",
      "Epoch: 0075 train_loss= 0.41385 time= 0.22652\n",
      "Epoch: 0076 train_loss= 0.41360 time= 0.22282\n",
      "Epoch: 0077 train_loss= 0.41335 time= 0.19986\n",
      "Epoch: 0078 train_loss= 0.41312 time= 0.20208\n",
      "Epoch: 0079 train_loss= 0.41289 time= 0.22447\n",
      "Epoch: 0080 train_loss= 0.41267 time= 0.22529\n",
      "Epoch: 0081 train_loss= 0.41246 time= 0.23267\n",
      "Epoch: 0082 train_loss= 0.41226 time= 0.20569\n",
      "Epoch: 0083 train_loss= 0.41207 time= 0.22923\n",
      "Epoch: 0084 train_loss= 0.41188 time= 0.20938\n",
      "Epoch: 0085 train_loss= 0.41170 time= 0.20709\n",
      "Epoch: 0086 train_loss= 0.41153 time= 0.20908\n",
      "Epoch: 0087 train_loss= 0.41137 time= 0.23348\n",
      "Epoch: 0088 train_loss= 0.41121 time= 0.21064\n",
      "Epoch: 0089 train_loss= 0.41106 time= 0.20115\n",
      "Epoch: 0090 train_loss= 0.41092 time= 0.22655\n",
      "Epoch: 0091 train_loss= 0.41078 time= 0.20725\n",
      "Epoch: 0092 train_loss= 0.41064 time= 0.22802\n",
      "Epoch: 0093 train_loss= 0.41052 time= 0.20433\n",
      "Epoch: 0094 train_loss= 0.41039 time= 0.22618\n",
      "Epoch: 0095 train_loss= 0.41028 time= 0.20914\n",
      "Epoch: 0096 train_loss= 0.41016 time= 0.22807\n",
      "Epoch: 0097 train_loss= 0.41006 time= 0.23150\n",
      "Epoch: 0098 train_loss= 0.40995 time= 0.20456\n",
      "Epoch: 0099 train_loss= 0.40985 time= 0.20238\n",
      "Epoch: 0100 train_loss= 0.40975 time= 0.21306\n",
      "Epoch: 0101 train_loss= 0.40966 time= 0.22354\n",
      "Epoch: 0102 train_loss= 0.40957 time= 0.22740\n",
      "Epoch: 0103 train_loss= 0.40948 time= 0.22799\n",
      "Epoch: 0104 train_loss= 0.40940 time= 0.22774\n",
      "Epoch: 0105 train_loss= 0.40932 time= 0.20224\n",
      "Epoch: 0106 train_loss= 0.40924 time= 0.22486\n",
      "Epoch: 0107 train_loss= 0.40917 time= 0.22763\n",
      "Epoch: 0108 train_loss= 0.40909 time= 0.21235\n",
      "Epoch: 0109 train_loss= 0.40902 time= 0.20860\n",
      "Epoch: 0110 train_loss= 0.40895 time= 0.22850\n",
      "Epoch: 0111 train_loss= 0.40889 time= 0.20163\n",
      "Epoch: 0112 train_loss= 0.40882 time= 0.20363\n",
      "Epoch: 0113 train_loss= 0.40876 time= 0.20362\n",
      "Epoch: 0114 train_loss= 0.40870 time= 0.20839\n",
      "Epoch: 0115 train_loss= 0.40864 time= 0.22217\n",
      "Epoch: 0116 train_loss= 0.40858 time= 0.20564\n",
      "Epoch: 0117 train_loss= 0.40853 time= 0.20928\n",
      "Epoch: 0118 train_loss= 0.40847 time= 0.20246\n",
      "Epoch: 0119 train_loss= 0.40842 time= 0.23758\n",
      "Epoch: 0120 train_loss= 0.40837 time= 0.23122\n",
      "Epoch: 0121 train_loss= 0.40832 time= 0.23376\n",
      "Epoch: 0122 train_loss= 0.40827 time= 0.23120\n",
      "Epoch: 0123 train_loss= 0.40822 time= 0.21086\n",
      "Epoch: 0124 train_loss= 0.40817 time= 0.20900\n",
      "Epoch: 0125 train_loss= 0.40813 time= 0.20956\n",
      "Epoch: 0126 train_loss= 0.40808 time= 0.20908\n",
      "Epoch: 0127 train_loss= 0.40804 time= 0.20021\n",
      "Epoch: 0128 train_loss= 0.40800 time= 0.23823\n",
      "Epoch: 0129 train_loss= 0.40795 time= 0.21157\n",
      "Epoch: 0130 train_loss= 0.40791 time= 0.22179\n",
      "Epoch: 0131 train_loss= 0.40787 time= 0.21018\n",
      "Epoch: 0132 train_loss= 0.40784 time= 0.22126\n",
      "Epoch: 0133 train_loss= 0.40780 time= 0.20731\n",
      "Epoch: 0134 train_loss= 0.40776 time= 0.22912\n",
      "Epoch: 0135 train_loss= 0.40772 time= 0.20911\n",
      "Epoch: 0136 train_loss= 0.40769 time= 0.22582\n",
      "Epoch: 0137 train_loss= 0.40765 time= 0.21506\n",
      "Epoch: 0138 train_loss= 0.40762 time= 0.20709\n",
      "Epoch: 0139 train_loss= 0.40758 time= 0.22084\n",
      "Epoch: 0140 train_loss= 0.40755 time= 0.21656\n",
      "Epoch: 0141 train_loss= 0.40752 time= 0.21110\n",
      "Epoch: 0142 train_loss= 0.40749 time= 0.20512\n",
      "Epoch: 0143 train_loss= 0.40746 time= 0.22751\n",
      "Epoch: 0144 train_loss= 0.40742 time= 0.22950\n",
      "Epoch: 0145 train_loss= 0.40739 time= 0.23198\n",
      "Epoch: 0146 train_loss= 0.40736 time= 0.22899\n",
      "Epoch: 0147 train_loss= 0.40734 time= 0.22950\n",
      "Epoch: 0148 train_loss= 0.40731 time= 0.23164\n",
      "Epoch: 0149 train_loss= 0.40728 time= 0.20958\n",
      "Epoch: 0150 train_loss= 0.40725 time= 0.20510\n",
      "Epoch: 0151 train_loss= 0.40722 time= 0.22698\n",
      "Epoch: 0152 train_loss= 0.40720 time= 0.22601\n",
      "Epoch: 0153 train_loss= 0.40717 time= 0.21934\n",
      "Epoch: 0154 train_loss= 0.40714 time= 0.22753\n",
      "Epoch: 0155 train_loss= 0.40712 time= 0.23200\n",
      "Epoch: 0156 train_loss= 0.40709 time= 0.20162\n",
      "Epoch: 0157 train_loss= 0.40707 time= 0.23471\n",
      "Epoch: 0158 train_loss= 0.40704 time= 0.22819\n",
      "Epoch: 0159 train_loss= 0.40702 time= 0.20694\n",
      "Epoch: 0160 train_loss= 0.40700 time= 0.20014\n",
      "Epoch: 0161 train_loss= 0.40697 time= 0.22254\n",
      "Epoch: 0162 train_loss= 0.40695 time= 0.21074\n",
      "Epoch: 0163 train_loss= 0.40693 time= 0.21009\n",
      "Epoch: 0164 train_loss= 0.40690 time= 0.20984\n",
      "Epoch: 0165 train_loss= 0.40688 time= 0.20808\n",
      "Epoch: 0166 train_loss= 0.40686 time= 0.20609\n",
      "Epoch: 0167 train_loss= 0.40684 time= 0.22389\n",
      "Epoch: 0168 train_loss= 0.40682 time= 0.23098\n",
      "Epoch: 0169 train_loss= 0.40679 time= 0.22202\n",
      "Epoch: 0170 train_loss= 0.40677 time= 0.22885\n",
      "Epoch: 0171 train_loss= 0.40675 time= 0.20559\n",
      "Epoch: 0172 train_loss= 0.40673 time= 0.20526\n",
      "Epoch: 0173 train_loss= 0.40671 time= 0.22752\n",
      "Epoch: 0174 train_loss= 0.40669 time= 0.20719\n",
      "Epoch: 0175 train_loss= 0.40667 time= 0.22369\n",
      "Epoch: 0176 train_loss= 0.40665 time= 0.20614\n",
      "Epoch: 0177 train_loss= 0.40663 time= 0.22202\n",
      "Epoch: 0178 train_loss= 0.40661 time= 0.20163\n",
      "Epoch: 0179 train_loss= 0.40659 time= 0.22854\n",
      "Epoch: 0180 train_loss= 0.40658 time= 0.20439\n",
      "Epoch: 0181 train_loss= 0.40656 time= 0.23351\n",
      "Epoch: 0182 train_loss= 0.40654 time= 0.22700\n",
      "Epoch: 0183 train_loss= 0.40652 time= 0.23198\n",
      "Epoch: 0184 train_loss= 0.40650 time= 0.20410\n",
      "Epoch: 0185 train_loss= 0.40648 time= 0.22353\n",
      "Epoch: 0186 train_loss= 0.40647 time= 0.20748\n",
      "Epoch: 0187 train_loss= 0.40645 time= 0.22261\n",
      "Epoch: 0188 train_loss= 0.40643 time= 0.20131\n",
      "Epoch: 0189 train_loss= 0.40641 time= 0.20540\n",
      "Epoch: 0190 train_loss= 0.40640 time= 0.20575\n",
      "Epoch: 0191 train_loss= 0.40638 time= 0.20365\n",
      "Epoch: 0192 train_loss= 0.40636 time= 0.20510\n",
      "Epoch: 0193 train_loss= 0.40635 time= 0.22800\n",
      "Epoch: 0194 train_loss= 0.40633 time= 0.23249\n",
      "Epoch: 0195 train_loss= 0.40631 time= 0.22800\n",
      "Epoch: 0196 train_loss= 0.40630 time= 0.22405\n",
      "Epoch: 0197 train_loss= 0.40628 time= 0.22723\n",
      "Epoch: 0198 train_loss= 0.40627 time= 0.22152\n",
      "Epoch: 0199 train_loss= 0.40625 time= 0.21041\n",
      "Epoch: 0200 train_loss= 0.40623 time= 0.20262\n",
      "Testing model...\n",
      "\n",
      "Test results for linear_ae model on cora on link_prediction \n",
      " ___________________________________________________\n",
      "\n",
      "AUC scores\n",
      " [0.8421806869286248]\n",
      "Mean AUC score:  0.8421806869286248 \n",
      "Std of AUC scores:  0.0 \n",
      " \n",
      "\n",
      "AP scores\n",
      " [0.885510817743201]\n",
      "Mean AP score:  0.885510817743201 \n",
      "Std of AP scores:  0.0 \n",
      " \n",
      "\n",
      "Total Running times\n",
      " [44.0699679851532]\n",
      "Mean total running time:  44.0699679851532 \n",
      "Std of total running time:  0.0 \n",
      " \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From train.py:177: The name tf.sparse_placeholder is deprecated. Please use tf.compat.v1.sparse_placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From train.py:180: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From e:\\project\\linear_graph_autoencoders\\linear_gae\\model.py:38: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From e:\\project\\linear_graph_autoencoders\\linear_gae\\initializations.py:14: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From e:\\project\\linear_graph_autoencoders\\linear_gae\\layers.py:29: The name tf.sparse_retain is deprecated. Please use tf.sparse.retain instead.\n",
      "\n",
      "WARNING:tensorflow:From e:\\project\\linear_graph_autoencoders\\linear_gae\\layers.py:101: The name tf.sparse_tensor_dense_matmul is deprecated. Please use tf.sparse.sparse_dense_matmul instead.\n",
      "\n",
      "WARNING:tensorflow:From e:\\project\\linear_graph_autoencoders\\linear_gae\\layers.py:115: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From e:\\project\\linear_graph_autoencoders\\linear_gae\\model.py:40: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From e:\\project\\linear_graph_autoencoders\\linear_gae\\model.py:40: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "WARNING:tensorflow:From train.py:217: The name tf.sparse_tensor_to_dense is deprecated. Please use tf.sparse.to_dense instead.\n",
      "\n",
      "WARNING:tensorflow:From e:\\project\\linear_graph_autoencoders\\linear_gae\\optimizer.py:20: calling weighted_cross_entropy_with_logits (from tensorflow.python.ops.nn_impl) with targets is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "targets is deprecated, use labels instead\n",
      "WARNING:tensorflow:From e:\\project\\linear_graph_autoencoders\\linear_gae\\optimizer.py:22: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From train.py:236: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From train.py:237: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python train.py --model=linear_ae --dataset=cora --task=link_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Masking test edges...\n",
      "Preprocessing and Initializing...\n",
      "Training...\n",
      "Epoch: 0001 train_loss= 0.79727 time= 0.31716\n",
      "Epoch: 0002 train_loss= 0.79627 time= 0.20709\n",
      "Epoch: 0003 train_loss= 0.79371 time= 0.20510\n",
      "Epoch: 0004 train_loss= 0.78871 time= 0.19901\n",
      "Epoch: 0005 train_loss= 0.78040 time= 0.20297\n",
      "Epoch: 0006 train_loss= 0.76820 time= 0.20834\n",
      "Epoch: 0007 train_loss= 0.75250 time= 0.20811\n",
      "Epoch: 0008 train_loss= 0.73555 time= 0.21258\n",
      "Epoch: 0009 train_loss= 0.72255 time= 0.21262\n",
      "Epoch: 0010 train_loss= 0.72028 time= 0.21110\n",
      "Epoch: 0011 train_loss= 0.72511 time= 0.20948\n",
      "Epoch: 0012 train_loss= 0.72342 time= 0.20811\n",
      "Epoch: 0013 train_loss= 0.71476 time= 0.21427\n",
      "Epoch: 0014 train_loss= 0.70451 time= 0.20761\n",
      "Epoch: 0015 train_loss= 0.69632 time= 0.20648\n",
      "Epoch: 0016 train_loss= 0.69075 time= 0.21205\n",
      "Epoch: 0017 train_loss= 0.68644 time= 0.21232\n",
      "Epoch: 0018 train_loss= 0.68175 time= 0.21656\n",
      "Epoch: 0019 train_loss= 0.67562 time= 0.21408\n",
      "Epoch: 0020 train_loss= 0.66764 time= 0.20812\n",
      "Epoch: 0021 train_loss= 0.65794 time= 0.21159\n",
      "Epoch: 0022 train_loss= 0.64695 time= 0.21206\n",
      "Epoch: 0023 train_loss= 0.63529 time= 0.20410\n",
      "Epoch: 0024 train_loss= 0.62352 time= 0.20791\n",
      "Epoch: 0025 train_loss= 0.61200 time= 0.21186\n",
      "Epoch: 0026 train_loss= 0.60082 time= 0.20609\n",
      "Epoch: 0027 train_loss= 0.58987 time= 0.20311\n",
      "Epoch: 0028 train_loss= 0.57910 time= 0.20760\n",
      "Epoch: 0029 train_loss= 0.56870 time= 0.21107\n",
      "Epoch: 0030 train_loss= 0.55913 time= 0.21207\n",
      "Epoch: 0031 train_loss= 0.55082 time= 0.20809\n",
      "Epoch: 0032 train_loss= 0.54396 time= 0.20691\n",
      "Epoch: 0033 train_loss= 0.53840 time= 0.21924\n",
      "Epoch: 0034 train_loss= 0.53374 time= 0.21212\n",
      "Epoch: 0035 train_loss= 0.52957 time= 0.20723\n",
      "Epoch: 0036 train_loss= 0.52565 time= 0.21011\n",
      "Epoch: 0037 train_loss= 0.52189 time= 0.21028\n",
      "Epoch: 0038 train_loss= 0.51825 time= 0.20609\n",
      "Epoch: 0039 train_loss= 0.51462 time= 0.21070\n",
      "Epoch: 0040 train_loss= 0.51079 time= 0.21110\n",
      "Epoch: 0041 train_loss= 0.50658 time= 0.20561\n",
      "Epoch: 0042 train_loss= 0.50201 time= 0.20598\n",
      "Epoch: 0043 train_loss= 0.49732 time= 0.20612\n",
      "Epoch: 0044 train_loss= 0.49285 time= 0.20712\n",
      "Epoch: 0045 train_loss= 0.48890 time= 0.20966\n",
      "Epoch: 0046 train_loss= 0.48561 time= 0.21206\n",
      "Epoch: 0047 train_loss= 0.48297 time= 0.20760\n",
      "Epoch: 0048 train_loss= 0.48086 time= 0.21064\n",
      "Epoch: 0049 train_loss= 0.47913 time= 0.20679\n",
      "Epoch: 0050 train_loss= 0.47764 time= 0.20709\n",
      "Epoch: 0051 train_loss= 0.47635 time= 0.20637\n",
      "Epoch: 0052 train_loss= 0.47521 time= 0.20810\n",
      "Epoch: 0053 train_loss= 0.47420 time= 0.20612\n",
      "Epoch: 0054 train_loss= 0.47329 time= 0.21358\n",
      "Epoch: 0055 train_loss= 0.47242 time= 0.20709\n",
      "Epoch: 0056 train_loss= 0.47153 time= 0.20413\n",
      "Epoch: 0057 train_loss= 0.47052 time= 0.21433\n",
      "Epoch: 0058 train_loss= 0.46935 time= 0.20911\n",
      "Epoch: 0059 train_loss= 0.46804 time= 0.20941\n",
      "Epoch: 0060 train_loss= 0.46666 time= 0.20954\n",
      "Epoch: 0061 train_loss= 0.46530 time= 0.21851\n",
      "Epoch: 0062 train_loss= 0.46404 time= 0.21306\n",
      "Epoch: 0063 train_loss= 0.46293 time= 0.21442\n",
      "Epoch: 0064 train_loss= 0.46196 time= 0.20242\n",
      "Epoch: 0065 train_loss= 0.46110 time= 0.20919\n",
      "Epoch: 0066 train_loss= 0.46032 time= 0.20985\n",
      "Epoch: 0067 train_loss= 0.45962 time= 0.20412\n",
      "Epoch: 0068 train_loss= 0.45896 time= 0.20563\n",
      "Epoch: 0069 train_loss= 0.45836 time= 0.21214\n",
      "Epoch: 0070 train_loss= 0.45780 time= 0.20542\n",
      "Epoch: 0071 train_loss= 0.45728 time= 0.20751\n",
      "Epoch: 0072 train_loss= 0.45677 time= 0.20821\n",
      "Epoch: 0073 train_loss= 0.45627 time= 0.20645\n",
      "Epoch: 0074 train_loss= 0.45577 time= 0.21009\n",
      "Epoch: 0075 train_loss= 0.45527 time= 0.21406\n",
      "Epoch: 0076 train_loss= 0.45476 time= 0.20712\n",
      "Epoch: 0077 train_loss= 0.45427 time= 0.20412\n",
      "Epoch: 0078 train_loss= 0.45378 time= 0.20682\n",
      "Epoch: 0079 train_loss= 0.45329 time= 0.20211\n",
      "Epoch: 0080 train_loss= 0.45280 time= 0.21207\n",
      "Epoch: 0081 train_loss= 0.45232 time= 0.20512\n",
      "Epoch: 0082 train_loss= 0.45184 time= 0.21109\n",
      "Epoch: 0083 train_loss= 0.45136 time= 0.20916\n",
      "Epoch: 0084 train_loss= 0.45089 time= 0.21257\n",
      "Epoch: 0085 train_loss= 0.45043 time= 0.21107\n",
      "Epoch: 0086 train_loss= 0.44997 time= 0.20808\n",
      "Epoch: 0087 train_loss= 0.44952 time= 0.21126\n",
      "Epoch: 0088 train_loss= 0.44907 time= 0.21909\n",
      "Epoch: 0089 train_loss= 0.44861 time= 0.21298\n",
      "Epoch: 0090 train_loss= 0.44814 time= 0.20998\n",
      "Epoch: 0091 train_loss= 0.44765 time= 0.20711\n",
      "Epoch: 0092 train_loss= 0.44716 time= 0.21432\n",
      "Epoch: 0093 train_loss= 0.44666 time= 0.21083\n",
      "Epoch: 0094 train_loss= 0.44615 time= 0.20808\n",
      "Epoch: 0095 train_loss= 0.44563 time= 0.20609\n",
      "Epoch: 0096 train_loss= 0.44510 time= 0.20808\n",
      "Epoch: 0097 train_loss= 0.44458 time= 0.21406\n",
      "Epoch: 0098 train_loss= 0.44405 time= 0.20765\n",
      "Epoch: 0099 train_loss= 0.44354 time= 0.20709\n",
      "Epoch: 0100 train_loss= 0.44303 time= 0.20709\n",
      "Epoch: 0101 train_loss= 0.44252 time= 0.20410\n",
      "Epoch: 0102 train_loss= 0.44203 time= 0.21111\n",
      "Epoch: 0103 train_loss= 0.44153 time= 0.20563\n",
      "Epoch: 0104 train_loss= 0.44105 time= 0.20811\n",
      "Epoch: 0105 train_loss= 0.44056 time= 0.21287\n",
      "Epoch: 0106 train_loss= 0.44009 time= 0.21015\n",
      "Epoch: 0107 train_loss= 0.43962 time= 0.20165\n",
      "Epoch: 0108 train_loss= 0.43915 time= 0.20624\n",
      "Epoch: 0109 train_loss= 0.43868 time= 0.21025\n",
      "Epoch: 0110 train_loss= 0.43822 time= 0.21209\n",
      "Epoch: 0111 train_loss= 0.43776 time= 0.20910\n",
      "Epoch: 0112 train_loss= 0.43731 time= 0.20908\n",
      "Epoch: 0113 train_loss= 0.43687 time= 0.20960\n",
      "Epoch: 0114 train_loss= 0.43645 time= 0.20415\n",
      "Epoch: 0115 train_loss= 0.43603 time= 0.20939\n",
      "Epoch: 0116 train_loss= 0.43562 time= 0.20709\n",
      "Epoch: 0117 train_loss= 0.43522 time= 0.21107\n",
      "Epoch: 0118 train_loss= 0.43484 time= 0.20908\n",
      "Epoch: 0119 train_loss= 0.43447 time= 0.20908\n",
      "Epoch: 0120 train_loss= 0.43410 time= 0.20461\n",
      "Epoch: 0121 train_loss= 0.43374 time= 0.20711\n",
      "Epoch: 0122 train_loss= 0.43338 time= 0.20810\n",
      "Epoch: 0123 train_loss= 0.43301 time= 0.21158\n",
      "Epoch: 0124 train_loss= 0.43264 time= 0.21895\n",
      "Epoch: 0125 train_loss= 0.43227 time= 0.21203\n",
      "Epoch: 0126 train_loss= 0.43189 time= 0.20939\n",
      "Epoch: 0127 train_loss= 0.43150 time= 0.20647\n",
      "Epoch: 0128 train_loss= 0.43110 time= 0.20211\n",
      "Epoch: 0129 train_loss= 0.43070 time= 0.20510\n",
      "Epoch: 0130 train_loss= 0.43030 time= 0.21406\n",
      "Epoch: 0131 train_loss= 0.42991 time= 0.21656\n",
      "Epoch: 0132 train_loss= 0.42952 time= 0.20959\n",
      "Epoch: 0133 train_loss= 0.42913 time= 0.20610\n",
      "Epoch: 0134 train_loss= 0.42876 time= 0.21007\n",
      "Epoch: 0135 train_loss= 0.42840 time= 0.20859\n",
      "Epoch: 0136 train_loss= 0.42804 time= 0.20959\n",
      "Epoch: 0137 train_loss= 0.42770 time= 0.20912\n",
      "Epoch: 0138 train_loss= 0.42737 time= 0.21349\n",
      "Epoch: 0139 train_loss= 0.42705 time= 0.20444\n",
      "Epoch: 0140 train_loss= 0.42674 time= 0.20447\n",
      "Epoch: 0141 train_loss= 0.42644 time= 0.20370\n",
      "Epoch: 0142 train_loss= 0.42615 time= 0.21377\n",
      "Epoch: 0143 train_loss= 0.42586 time= 0.21206\n",
      "Epoch: 0144 train_loss= 0.42558 time= 0.21386\n",
      "Epoch: 0145 train_loss= 0.42530 time= 0.20836\n",
      "Epoch: 0146 train_loss= 0.42503 time= 0.20916\n",
      "Epoch: 0147 train_loss= 0.42476 time= 0.20563\n",
      "Epoch: 0148 train_loss= 0.42449 time= 0.21290\n",
      "Epoch: 0149 train_loss= 0.42422 time= 0.20822\n",
      "Epoch: 0150 train_loss= 0.42396 time= 0.21310\n",
      "Epoch: 0151 train_loss= 0.42369 time= 0.21064\n",
      "Epoch: 0152 train_loss= 0.42342 time= 0.20848\n",
      "Epoch: 0153 train_loss= 0.42316 time= 0.21212\n",
      "Epoch: 0154 train_loss= 0.42289 time= 0.21290\n",
      "Epoch: 0155 train_loss= 0.42262 time= 0.21309\n",
      "Epoch: 0156 train_loss= 0.42235 time= 0.21765\n",
      "Epoch: 0157 train_loss= 0.42208 time= 0.20809\n",
      "Epoch: 0158 train_loss= 0.42181 time= 0.20622\n",
      "Epoch: 0159 train_loss= 0.42153 time= 0.21208\n",
      "Epoch: 0160 train_loss= 0.42125 time= 0.21868\n",
      "Epoch: 0161 train_loss= 0.42097 time= 0.20883\n",
      "Epoch: 0162 train_loss= 0.42070 time= 0.21657\n",
      "Epoch: 0163 train_loss= 0.42042 time= 0.21270\n",
      "Epoch: 0164 train_loss= 0.42014 time= 0.21108\n",
      "Epoch: 0165 train_loss= 0.41986 time= 0.21207\n",
      "Epoch: 0166 train_loss= 0.41959 time= 0.22175\n",
      "Epoch: 0167 train_loss= 0.41932 time= 0.21007\n",
      "Epoch: 0168 train_loss= 0.41905 time= 0.21187\n",
      "Epoch: 0169 train_loss= 0.41879 time= 0.20601\n",
      "Epoch: 0170 train_loss= 0.41853 time= 0.20531\n",
      "Epoch: 0171 train_loss= 0.41828 time= 0.21468\n",
      "Epoch: 0172 train_loss= 0.41803 time= 0.21210\n",
      "Epoch: 0173 train_loss= 0.41779 time= 0.20786\n",
      "Epoch: 0174 train_loss= 0.41756 time= 0.21007\n",
      "Epoch: 0175 train_loss= 0.41733 time= 0.22662\n",
      "Epoch: 0176 train_loss= 0.41711 time= 0.21389\n",
      "Epoch: 0177 train_loss= 0.41690 time= 0.20526\n",
      "Epoch: 0178 train_loss= 0.41669 time= 0.20870\n",
      "Epoch: 0179 train_loss= 0.41648 time= 0.20908\n",
      "Epoch: 0180 train_loss= 0.41628 time= 0.20996\n",
      "Epoch: 0181 train_loss= 0.41608 time= 0.21279\n",
      "Epoch: 0182 train_loss= 0.41589 time= 0.21161\n",
      "Epoch: 0183 train_loss= 0.41569 time= 0.20681\n",
      "Epoch: 0184 train_loss= 0.41550 time= 0.20683\n",
      "Epoch: 0185 train_loss= 0.41532 time= 0.21332\n",
      "Epoch: 0186 train_loss= 0.41513 time= 0.20843\n",
      "Epoch: 0187 train_loss= 0.41495 time= 0.20829\n",
      "Epoch: 0188 train_loss= 0.41477 time= 0.20863\n",
      "Epoch: 0189 train_loss= 0.41459 time= 0.20937\n",
      "Epoch: 0190 train_loss= 0.41441 time= 0.21673\n",
      "Epoch: 0191 train_loss= 0.41424 time= 0.20591\n",
      "Epoch: 0192 train_loss= 0.41406 time= 0.20166\n",
      "Epoch: 0193 train_loss= 0.41389 time= 0.20725\n",
      "Epoch: 0194 train_loss= 0.41372 time= 0.21491\n",
      "Epoch: 0195 train_loss= 0.41355 time= 0.20962\n",
      "Epoch: 0196 train_loss= 0.41339 time= 0.22157\n",
      "Epoch: 0197 train_loss= 0.41322 time= 0.21095\n",
      "Epoch: 0198 train_loss= 0.41305 time= 0.21360\n",
      "Epoch: 0199 train_loss= 0.41289 time= 0.21258\n",
      "Epoch: 0200 train_loss= 0.41272 time= 0.21840\n",
      "Testing model...\n",
      "\n",
      "Test results for gcn_ae model on cora on link_prediction \n",
      " ___________________________________________________\n",
      "\n",
      "AUC scores\n",
      " [0.8520482196673737]\n",
      "Mean AUC score:  0.8520482196673737 \n",
      "Std of AUC scores:  0.0 \n",
      " \n",
      "\n",
      "AP scores\n",
      " [0.8940884953930358]\n",
      "Mean AP score:  0.8940884953930358 \n",
      "Std of AP scores:  0.0 \n",
      " \n",
      "\n",
      "Total Running times\n",
      " [42.47228217124939]\n",
      "Mean total running time:  42.47228217124939 \n",
      "Std of total running time:  0.0 \n",
      " \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From train.py:177: The name tf.sparse_placeholder is deprecated. Please use tf.compat.v1.sparse_placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From train.py:180: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From e:\\project\\linear_graph_autoencoders\\linear_gae\\model.py:38: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From e:\\project\\linear_graph_autoencoders\\linear_gae\\initializations.py:14: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From e:\\project\\linear_graph_autoencoders\\linear_gae\\layers.py:29: The name tf.sparse_retain is deprecated. Please use tf.sparse.retain instead.\n",
      "\n",
      "WARNING:tensorflow:From e:\\project\\linear_graph_autoencoders\\linear_gae\\layers.py:101: The name tf.sparse_tensor_dense_matmul is deprecated. Please use tf.sparse.sparse_dense_matmul instead.\n",
      "\n",
      "WARNING:tensorflow:From e:\\project\\linear_graph_autoencoders\\linear_gae\\layers.py:79: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From e:\\project\\linear_graph_autoencoders\\linear_gae\\model.py:40: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From e:\\project\\linear_graph_autoencoders\\linear_gae\\model.py:40: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "WARNING:tensorflow:From train.py:217: The name tf.sparse_tensor_to_dense is deprecated. Please use tf.sparse.to_dense instead.\n",
      "\n",
      "WARNING:tensorflow:From e:\\project\\linear_graph_autoencoders\\linear_gae\\optimizer.py:20: calling weighted_cross_entropy_with_logits (from tensorflow.python.ops.nn_impl) with targets is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "targets is deprecated, use labels instead\n",
      "WARNING:tensorflow:From e:\\project\\linear_graph_autoencoders\\linear_gae\\optimizer.py:22: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From train.py:236: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From train.py:237: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python train.py --model=gcn_ae --dataset=cora --task=link_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Masking test edges...\n",
      "Preprocessing and Initializing...\n",
      "WARNING:tensorflow:From E:\\jupyter\\linear_graph_autoencoders\\linear_gae\\train.py:177: The name tf.sparse_placeholder is deprecated. Please use tf.compat.v1.sparse_placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From E:\\jupyter\\linear_graph_autoencoders\\linear_gae\\train.py:180: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From e:\\project\\linear_graph_autoencoders\\linear_gae\\model.py:38: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From e:\\project\\linear_graph_autoencoders\\linear_gae\\initializations.py:14: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From e:\\project\\linear_graph_autoencoders\\linear_gae\\layers.py:29: The name tf.sparse_retain is deprecated. Please use tf.sparse.retain instead.\n",
      "\n",
      "WARNING:tensorflow:From e:\\project\\linear_graph_autoencoders\\linear_gae\\layers.py:101: The name tf.sparse_tensor_dense_matmul is deprecated. Please use tf.sparse.sparse_dense_matmul instead.\n",
      "\n",
      "WARNING:tensorflow:From e:\\project\\linear_graph_autoencoders\\linear_gae\\layers.py:79: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From e:\\project\\linear_graph_autoencoders\\linear_gae\\model.py:40: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From e:\\project\\linear_graph_autoencoders\\linear_gae\\model.py:40: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "WARNING:tensorflow:From E:\\jupyter\\linear_graph_autoencoders\\linear_gae\\train.py:217: The name tf.sparse_tensor_to_dense is deprecated. Please use tf.sparse.to_dense instead.\n",
      "\n",
      "WARNING:tensorflow:From e:\\project\\linear_graph_autoencoders\\linear_gae\\optimizer.py:20: calling weighted_cross_entropy_with_logits (from tensorflow.python.ops.nn_impl) with targets is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "targets is deprecated, use labels instead\n",
      "WARNING:tensorflow:From e:\\project\\linear_graph_autoencoders\\linear_gae\\optimizer.py:22: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From E:\\jupyter\\linear_graph_autoencoders\\linear_gae\\train.py:236: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From E:\\jupyter\\linear_graph_autoencoders\\linear_gae\\train.py:237: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "Training...\n",
      "Epoch: 0001 train_loss= 0.79736 time= 0.34182\n",
      "Epoch: 0002 train_loss= 0.79668 time= 0.23609\n",
      "Epoch: 0003 train_loss= 0.79494 time= 0.23950\n",
      "Epoch: 0004 train_loss= 0.79155 time= 0.23678\n",
      "Epoch: 0005 train_loss= 0.78580 time= 0.22895\n",
      "Epoch: 0006 train_loss= 0.77701 time= 0.22929\n",
      "Epoch: 0007 train_loss= 0.76480 time= 0.23403\n",
      "Epoch: 0008 train_loss= 0.74946 time= 0.23710\n",
      "Epoch: 0009 train_loss= 0.73275 time= 0.22560\n",
      "Epoch: 0010 train_loss= 0.71861 time= 0.22023\n",
      "Epoch: 0011 train_loss= 0.71268 time= 0.21196\n",
      "Epoch: 0012 train_loss= 0.71492 time= 0.21305\n",
      "Epoch: 0013 train_loss= 0.71431 time= 0.20510\n",
      "Epoch: 0014 train_loss= 0.70648 time= 0.20362\n",
      "Epoch: 0015 train_loss= 0.69491 time= 0.21117\n",
      "Epoch: 0016 train_loss= 0.68365 time= 0.21473\n",
      "Epoch: 0017 train_loss= 0.67448 time= 0.20828\n",
      "Epoch: 0018 train_loss= 0.66698 time= 0.21558\n",
      "Epoch: 0019 train_loss= 0.65971 time= 0.21804\n",
      "Epoch: 0020 train_loss= 0.65137 time= 0.21606\n",
      "Epoch: 0021 train_loss= 0.64129 time= 0.21554\n",
      "Epoch: 0022 train_loss= 0.62939 time= 0.20757\n",
      "Epoch: 0023 train_loss= 0.61607 time= 0.21693\n",
      "Epoch: 0024 train_loss= 0.60208 time= 0.21813\n",
      "Epoch: 0025 train_loss= 0.58831 time= 0.21258\n",
      "Epoch: 0026 train_loss= 0.57568 time= 0.21176\n",
      "Epoch: 0027 train_loss= 0.56487 time= 0.21536\n",
      "Epoch: 0028 train_loss= 0.55616 time= 0.21007\n",
      "Epoch: 0029 train_loss= 0.54944 time= 0.21278\n",
      "Epoch: 0030 train_loss= 0.54435 time= 0.20962\n",
      "Epoch: 0031 train_loss= 0.54046 time= 0.20561\n",
      "Epoch: 0032 train_loss= 0.53739 time= 0.22700\n",
      "Epoch: 0033 train_loss= 0.53472 time= 0.21619\n",
      "Epoch: 0034 train_loss= 0.53202 time= 0.20827\n",
      "Epoch: 0035 train_loss= 0.52888 time= 0.22003\n",
      "Epoch: 0036 train_loss= 0.52509 time= 0.22047\n",
      "Epoch: 0037 train_loss= 0.52069 time= 0.21704\n",
      "Epoch: 0038 train_loss= 0.51592 time= 0.21903\n",
      "Epoch: 0039 train_loss= 0.51110 time= 0.21324\n",
      "Epoch: 0040 train_loss= 0.50653 time= 0.21197\n",
      "Epoch: 0041 train_loss= 0.50239 time= 0.20908\n",
      "Epoch: 0042 train_loss= 0.49874 time= 0.21327\n",
      "Epoch: 0043 train_loss= 0.49558 time= 0.21357\n",
      "Epoch: 0044 train_loss= 0.49284 time= 0.20852\n",
      "Epoch: 0045 train_loss= 0.49044 time= 0.22069\n",
      "Epoch: 0046 train_loss= 0.48829 time= 0.21392\n",
      "Epoch: 0047 train_loss= 0.48627 time= 0.22119\n",
      "Epoch: 0048 train_loss= 0.48428 time= 0.21076\n",
      "Epoch: 0049 train_loss= 0.48224 time= 0.21598\n",
      "Epoch: 0050 train_loss= 0.48008 time= 0.21261\n",
      "Epoch: 0051 train_loss= 0.47779 time= 0.20750\n",
      "Epoch: 0052 train_loss= 0.47539 time= 0.21685\n",
      "Epoch: 0053 train_loss= 0.47293 time= 0.22219\n",
      "Epoch: 0054 train_loss= 0.47050 time= 0.21874\n",
      "Epoch: 0055 train_loss= 0.46816 time= 0.22442\n",
      "Epoch: 0056 train_loss= 0.46598 time= 0.21657\n",
      "Epoch: 0057 train_loss= 0.46402 time= 0.22282\n",
      "Epoch: 0058 train_loss= 0.46228 time= 0.21601\n",
      "Epoch: 0059 train_loss= 0.46078 time= 0.21292\n",
      "Epoch: 0060 train_loss= 0.45951 time= 0.21308\n",
      "Epoch: 0061 train_loss= 0.45845 time= 0.22011\n",
      "Epoch: 0062 train_loss= 0.45755 time= 0.21262\n",
      "Epoch: 0063 train_loss= 0.45675 time= 0.21505\n",
      "Epoch: 0064 train_loss= 0.45598 time= 0.21190\n",
      "Epoch: 0065 train_loss= 0.45517 time= 0.21330\n",
      "Epoch: 0066 train_loss= 0.45431 time= 0.21617\n",
      "Epoch: 0067 train_loss= 0.45338 time= 0.21915\n",
      "Epoch: 0068 train_loss= 0.45243 time= 0.21690\n",
      "Epoch: 0069 train_loss= 0.45148 time= 0.21428\n",
      "Epoch: 0070 train_loss= 0.45057 time= 0.20745\n",
      "Epoch: 0071 train_loss= 0.44970 time= 0.20854\n",
      "Epoch: 0072 train_loss= 0.44887 time= 0.21384\n",
      "Epoch: 0073 train_loss= 0.44810 time= 0.21209\n",
      "Epoch: 0074 train_loss= 0.44737 time= 0.20311\n",
      "Epoch: 0075 train_loss= 0.44670 time= 0.20515\n",
      "Epoch: 0076 train_loss= 0.44609 time= 0.20810\n",
      "Epoch: 0077 train_loss= 0.44555 time= 0.21211\n",
      "Epoch: 0078 train_loss= 0.44507 time= 0.21272\n",
      "Epoch: 0079 train_loss= 0.44465 time= 0.21683\n",
      "Epoch: 0080 train_loss= 0.44425 time= 0.21929\n",
      "Epoch: 0081 train_loss= 0.44386 time= 0.21556\n",
      "Epoch: 0082 train_loss= 0.44346 time= 0.21614\n",
      "Epoch: 0083 train_loss= 0.44305 time= 0.21704\n",
      "Epoch: 0084 train_loss= 0.44261 time= 0.20787\n",
      "Epoch: 0085 train_loss= 0.44216 time= 0.21623\n",
      "Epoch: 0086 train_loss= 0.44170 time= 0.21357\n",
      "Epoch: 0087 train_loss= 0.44123 time= 0.21814\n",
      "Epoch: 0088 train_loss= 0.44076 time= 0.21008\n",
      "Epoch: 0089 train_loss= 0.44028 time= 0.21561\n",
      "Epoch: 0090 train_loss= 0.43979 time= 0.21441\n",
      "Epoch: 0091 train_loss= 0.43931 time= 0.21389\n",
      "Epoch: 0092 train_loss= 0.43882 time= 0.21699\n",
      "Epoch: 0093 train_loss= 0.43833 time= 0.21309\n",
      "Epoch: 0094 train_loss= 0.43785 time= 0.21906\n",
      "Epoch: 0095 train_loss= 0.43737 time= 0.21357\n",
      "Epoch: 0096 train_loss= 0.43689 time= 0.21261\n",
      "Epoch: 0097 train_loss= 0.43640 time= 0.22202\n",
      "Epoch: 0098 train_loss= 0.43592 time= 0.21107\n",
      "Epoch: 0099 train_loss= 0.43544 time= 0.21007\n",
      "Epoch: 0100 train_loss= 0.43496 time= 0.21505\n",
      "Epoch: 0101 train_loss= 0.43450 time= 0.21159\n",
      "Epoch: 0102 train_loss= 0.43404 time= 0.21306\n",
      "Epoch: 0103 train_loss= 0.43359 time= 0.21196\n",
      "Epoch: 0104 train_loss= 0.43315 time= 0.21242\n",
      "Epoch: 0105 train_loss= 0.43272 time= 0.21459\n",
      "Epoch: 0106 train_loss= 0.43230 time= 0.22729\n",
      "Epoch: 0107 train_loss= 0.43189 time= 0.21605\n",
      "Epoch: 0108 train_loss= 0.43149 time= 0.21881\n",
      "Epoch: 0109 train_loss= 0.43109 time= 0.21107\n",
      "Epoch: 0110 train_loss= 0.43070 time= 0.21008\n",
      "Epoch: 0111 train_loss= 0.43030 time= 0.21919\n",
      "Epoch: 0112 train_loss= 0.42992 time= 0.21015\n",
      "Epoch: 0113 train_loss= 0.42953 time= 0.21457\n",
      "Epoch: 0114 train_loss= 0.42916 time= 0.21512\n",
      "Epoch: 0115 train_loss= 0.42879 time= 0.21798\n",
      "Epoch: 0116 train_loss= 0.42843 time= 0.21255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0117 train_loss= 0.42807 time= 0.21217\n",
      "Epoch: 0118 train_loss= 0.42772 time= 0.21857\n",
      "Epoch: 0119 train_loss= 0.42737 time= 0.21139\n",
      "Epoch: 0120 train_loss= 0.42703 time= 0.21800\n",
      "Epoch: 0121 train_loss= 0.42668 time= 0.22101\n",
      "Epoch: 0122 train_loss= 0.42635 time= 0.21163\n",
      "Epoch: 0123 train_loss= 0.42601 time= 0.21256\n",
      "Epoch: 0124 train_loss= 0.42567 time= 0.21627\n",
      "Epoch: 0125 train_loss= 0.42534 time= 0.21410\n",
      "Epoch: 0126 train_loss= 0.42502 time= 0.21415\n",
      "Epoch: 0127 train_loss= 0.42470 time= 0.20989\n",
      "Epoch: 0128 train_loss= 0.42439 time= 0.21014\n",
      "Epoch: 0129 train_loss= 0.42408 time= 0.21757\n",
      "Epoch: 0130 train_loss= 0.42378 time= 0.21243\n",
      "Epoch: 0131 train_loss= 0.42348 time= 0.22103\n",
      "Epoch: 0132 train_loss= 0.42318 time= 0.21556\n",
      "Epoch: 0133 train_loss= 0.42289 time= 0.21804\n",
      "Epoch: 0134 train_loss= 0.42260 time= 0.21306\n",
      "Epoch: 0135 train_loss= 0.42231 time= 0.22003\n",
      "Epoch: 0136 train_loss= 0.42202 time= 0.21306\n",
      "Epoch: 0137 train_loss= 0.42174 time= 0.20909\n",
      "Epoch: 0138 train_loss= 0.42145 time= 0.21814\n",
      "Epoch: 0139 train_loss= 0.42117 time= 0.21061\n",
      "Epoch: 0140 train_loss= 0.42090 time= 0.21340\n",
      "Epoch: 0141 train_loss= 0.42064 time= 0.21248\n",
      "Epoch: 0142 train_loss= 0.42038 time= 0.21381\n",
      "Epoch: 0143 train_loss= 0.42013 time= 0.20944\n",
      "Epoch: 0144 train_loss= 0.41989 time= 0.20908\n",
      "Epoch: 0145 train_loss= 0.41965 time= 0.21086\n",
      "Epoch: 0146 train_loss= 0.41942 time= 0.20912\n",
      "Epoch: 0147 train_loss= 0.41920 time= 0.21605\n",
      "Epoch: 0148 train_loss= 0.41897 time= 0.20882\n",
      "Epoch: 0149 train_loss= 0.41875 time= 0.20479\n",
      "Epoch: 0150 train_loss= 0.41853 time= 0.21549\n",
      "Epoch: 0151 train_loss= 0.41830 time= 0.21264\n",
      "Epoch: 0152 train_loss= 0.41808 time= 0.20787\n",
      "Epoch: 0153 train_loss= 0.41785 time= 0.21257\n",
      "Epoch: 0154 train_loss= 0.41763 time= 0.21163\n",
      "Epoch: 0155 train_loss= 0.41741 time= 0.23072\n",
      "Epoch: 0156 train_loss= 0.41718 time= 0.20915\n",
      "Epoch: 0157 train_loss= 0.41697 time= 0.21510\n",
      "Epoch: 0158 train_loss= 0.41676 time= 0.21694\n",
      "Epoch: 0159 train_loss= 0.41655 time= 0.22384\n",
      "Epoch: 0160 train_loss= 0.41635 time= 0.23694\n",
      "Epoch: 0161 train_loss= 0.41616 time= 0.22491\n",
      "Epoch: 0162 train_loss= 0.41598 time= 0.23405\n",
      "Epoch: 0163 train_loss= 0.41581 time= 0.23467\n",
      "Epoch: 0164 train_loss= 0.41565 time= 0.23578\n",
      "Epoch: 0165 train_loss= 0.41550 time= 0.23162\n",
      "Epoch: 0166 train_loss= 0.41535 time= 0.23357\n",
      "Epoch: 0167 train_loss= 0.41521 time= 0.23487\n",
      "Epoch: 0168 train_loss= 0.41508 time= 0.24039\n",
      "Epoch: 0169 train_loss= 0.41496 time= 0.23257\n",
      "Epoch: 0170 train_loss= 0.41484 time= 0.23081\n",
      "Epoch: 0171 train_loss= 0.41472 time= 0.22700\n",
      "Epoch: 0172 train_loss= 0.41461 time= 0.23198\n",
      "Epoch: 0173 train_loss= 0.41450 time= 0.23274\n",
      "Epoch: 0174 train_loss= 0.41440 time= 0.23808\n",
      "Epoch: 0175 train_loss= 0.41430 time= 0.23252\n",
      "Epoch: 0176 train_loss= 0.41419 time= 0.22906\n",
      "Epoch: 0177 train_loss= 0.41409 time= 0.22064\n",
      "Epoch: 0178 train_loss= 0.41399 time= 0.21386\n",
      "Epoch: 0179 train_loss= 0.41389 time= 0.21107\n",
      "Epoch: 0180 train_loss= 0.41378 time= 0.21215\n",
      "Epoch: 0181 train_loss= 0.41368 time= 0.21456\n",
      "Epoch: 0182 train_loss= 0.41357 time= 0.21395\n",
      "Epoch: 0183 train_loss= 0.41347 time= 0.21552\n",
      "Epoch: 0184 train_loss= 0.41336 time= 0.22148\n",
      "Epoch: 0185 train_loss= 0.41325 time= 0.21505\n",
      "Epoch: 0186 train_loss= 0.41313 time= 0.21471\n",
      "Epoch: 0187 train_loss= 0.41302 time= 0.20979\n",
      "Epoch: 0188 train_loss= 0.41290 time= 0.20530\n",
      "Epoch: 0189 train_loss= 0.41278 time= 0.21587\n",
      "Epoch: 0190 train_loss= 0.41265 time= 0.21945\n",
      "Epoch: 0191 train_loss= 0.41253 time= 0.21498\n",
      "Epoch: 0192 train_loss= 0.41240 time= 0.21308\n",
      "Epoch: 0193 train_loss= 0.41227 time= 0.21154\n",
      "Epoch: 0194 train_loss= 0.41213 time= 0.21399\n",
      "Epoch: 0195 train_loss= 0.41200 time= 0.21536\n",
      "Epoch: 0196 train_loss= 0.41186 time= 0.22003\n",
      "Epoch: 0197 train_loss= 0.41172 time= 0.21459\n",
      "Epoch: 0198 train_loss= 0.41159 time= 0.21459\n",
      "Epoch: 0199 train_loss= 0.41145 time= 0.21873\n",
      "Epoch: 0200 train_loss= 0.41131 time= 0.21474\n",
      "Testing model...\n",
      "\n",
      "Test results for gcn_ae model on cora on link_prediction \n",
      " ___________________________________________________\n",
      "\n",
      "AUC scores\n",
      " [0.8486870294423701]\n",
      "Mean AUC score:  0.8486870294423701 \n",
      "Std of AUC scores:  0.0 \n",
      " \n",
      "\n",
      "AP scores\n",
      " [0.884645965892661]\n",
      "Mean AP score:  0.884645965892661 \n",
      "Std of AP scores:  0.0 \n",
      " \n",
      "\n",
      "Total Running times\n",
      " [44.168302059173584]\n",
      "Mean total running time:  44.168302059173584 \n",
      "Std of total running time:  0.0 \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run train.py --model=gcn_ae --dataset=cora --task=link_prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
